{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fc11122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyod\n",
      "  Downloading pyod-1.1.3.tar.gz (160 kB)\n",
      "     ---------------------------------------- 0.0/160.5 kB ? eta -:--:--\n",
      "     ----------------- --------------------- 71.7/160.5 kB 1.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ - 153.6/160.5 kB 1.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- 160.5/160.5 kB 1.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: joblib in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from pyod) (1.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from pyod) (3.7.2)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from pyod) (1.24.3)\n",
      "Requirement already satisfied: numba>=0.51 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from pyod) (0.57.1)\n",
      "Requirement already satisfied: scipy>=1.5.1 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from pyod) (1.11.1)\n",
      "Requirement already satisfied: scikit_learn>=0.22.0 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from pyod) (1.3.0)\n",
      "Requirement already satisfied: six in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from pyod) (1.16.0)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from numba>=0.51->pyod) (0.40.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit_learn>=0.22.0->pyod) (2.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib->pyod) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib->pyod) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib->pyod) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib->pyod) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib->pyod) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib->pyod) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib->pyod) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib->pyod) (2.8.2)\n",
      "Building wheels for collected packages: pyod\n",
      "  Building wheel for pyod (setup.py): started\n",
      "  Building wheel for pyod (setup.py): finished with status 'done'\n",
      "  Created wheel for pyod: filename=pyod-1.1.3-py3-none-any.whl size=190393 sha256=db00c521bce910b17910e5a559365488a3d2751147883d11b00a416ad7c32ac7\n",
      "  Stored in directory: c:\\users\\nhanamasagar\\appdata\\local\\pip\\cache\\wheels\\86\\9b\\f4\\bf3eea6cbc128fd0e5f871486beb8b657486f343d7f6ecbde9\n",
      "Successfully built pyod\n",
      "Installing collected packages: pyod\n",
      "Successfully installed pyod-1.1.3\n"
     ]
    }
   ],
   "source": [
    "# Install pyod library\n",
    "!pip install pyod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f739a63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/e4/14/d795bb156f8cc10eb1dcfe1332b7dbb8405b634688980aa9be8f885cc888/tensorflow-2.16.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorflow-2.16.1-cp311-cp311-win_amd64.whl.metadata (3.5 kB)\n",
      "Collecting tensorflow-intel==2.16.1 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-intel==2.16.1 from https://files.pythonhosted.org/packages/e0/36/6278e4e7e69a90c00e0f82944d8f2713dd85a69d1add455d9e50446837ab/tensorflow_intel-2.16.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorflow_intel-2.16.1-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl.metadata\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for astunparse>=1.6.0 from https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for flatbuffers>=23.5.26 from https://files.pythonhosted.org/packages/41/f0/7e988a019bc54b2dbd0ad4182ef2d53488bb02e58694cd79d61369e85900/flatbuffers-24.3.25-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 from https://files.pythonhosted.org/packages/fa/39/5aae571e5a5f4de9c3445dae08a530498e5c53b0e74410eeeb0991c79047/gast-0.5.4-py3-none-any.whl.metadata\n",
      "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for google-pasta>=0.1.1 from https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl.metadata\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for h5py>=3.10.0 from https://files.pythonhosted.org/packages/b6/35/ed21094eb4d8acf31ccc7666a4d8701c1ce38f8d1fa3c7036f24416f6337/h5py-3.10.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading h5py-3.10.0-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/0b/2d/3f480b1e1d31eb3d6de5e3ef641954e5c67430d5ac93b7fa7e07589576c7/libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for ml-dtypes~=0.3.1 from https://files.pythonhosted.org/packages/a4/db/1784b87285588788170f87e987bfb4bda218d62a70a81ebb66c94e7f9b95/ml_dtypes-0.3.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading ml_dtypes-0.3.2-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for opt-einsum>=2.3.2 from https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl.metadata\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/ad/6e/1bed3b7c904cc178cb8ee8dbaf72934964452b3de95b7a63412591edb93c/protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for termcolor>=1.1.0 from https://files.pythonhosted.org/packages/d9/5f/8c716e47b3a50cbd7c146f45881e11d9414def768b7cd9c5e6650ec2a80a/termcolor-2.4.0-py3-none-any.whl.metadata\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/78/a9/eaa378e6fe421c2f61bdd4b92439b2b8bb320526f2b0e08fcf4e21c2f855/grpcio-1.62.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading grpcio-1.62.1-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for tensorboard<2.17,>=2.16 from https://files.pythonhosted.org/packages/3a/d0/b97889ffa769e2d1fdebb632084d5e8b53fc299d43a537acee7ec0c021a3/tensorboard-2.16.2-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for keras>=3.0.0 from https://files.pythonhosted.org/packages/59/a8/d94e8acb59d678d908fe1db0c7ad89dfa2c2e2e529eeb3c2b3cc218a758d/keras-3.1.1-py3-none-any.whl.metadata\n",
      "  Downloading keras-3.1.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/ac/4e/9566a313927be582ca99455a9523a097c7888fc819695bdc08415432b202/tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.38.4)\n",
      "Collecting rich (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for rich from https://files.pythonhosted.org/packages/87/67/a37f6214d0e9fe57f6ae54b2956d550ca8365857f42a1ce0392bb21d9410/rich-13.7.1-py3-none-any.whl.metadata\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for namex from https://files.pythonhosted.org/packages/cd/43/b971880e2eb45c0bee2093710ae8044764a89afe9620df34a231c6f0ecd2/namex-0.0.7-py3-none-any.whl.metadata\n",
      "  Downloading namex-0.0.7-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for optree from https://files.pythonhosted.org/packages/8f/db/e05a35451d4ba30fdc65ef168dfdc68a6939ea6afdc0101e3e77f97e1547/optree-0.11.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading optree-0.11.0-cp311-cp311-win_amd64.whl.metadata (46 kB)\n",
      "     ---------------------------------------- 0.0/46.2 kB ? eta -:--:--\n",
      "     ----------------------------------- ---- 41.0/46.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 46.2/46.2 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/7a/13/e503968fefabd4c6b2650af21e110aa8466fe21432cd7c43a84577a89438/tensorboard_data_server-0.7.2-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\nhanamasagar\\appdata\\local\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.16.1-cp311-cp311-win_amd64.whl (2.1 kB)\n",
      "Downloading tensorflow_intel-2.16.1-cp311-cp311-win_amd64.whl (377.0 MB)\n",
      "   ---------------------------------------- 0.0/377.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.2/377.0 MB 6.3 MB/s eta 0:01:01\n",
      "   ---------------------------------------- 0.6/377.0 MB 8.0 MB/s eta 0:00:47\n",
      "   ---------------------------------------- 1.6/377.0 MB 12.9 MB/s eta 0:00:30\n",
      "   ---------------------------------------- 2.3/377.0 MB 14.6 MB/s eta 0:00:26\n",
      "   ---------------------------------------- 3.3/377.0 MB 16.3 MB/s eta 0:00:23\n",
      "   ---------------------------------------- 4.4/377.0 MB 17.5 MB/s eta 0:00:22\n",
      "    --------------------------------------- 5.3/377.0 MB 17.0 MB/s eta 0:00:22\n",
      "    --------------------------------------- 6.4/377.0 MB 19.4 MB/s eta 0:00:20\n",
      "    --------------------------------------- 7.7/377.0 MB 20.4 MB/s eta 0:00:19\n",
      "    --------------------------------------- 8.7/377.0 MB 19.8 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 9.6/377.0 MB 20.4 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 10.6/377.0 MB 22.6 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 11.2/377.0 MB 22.6 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 11.9/377.0 MB 21.8 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 13.2/377.0 MB 23.4 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 14.1/377.0 MB 23.4 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 15.5/377.0 MB 24.2 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 16.5/377.0 MB 24.2 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 17.7/377.0 MB 24.2 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 19.2/377.0 MB 25.2 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 19.9/377.0 MB 24.2 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 20.1/377.0 MB 23.4 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 20.5/377.0 MB 21.1 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 21.0/377.0 MB 21.8 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 21.3/377.0 MB 20.5 MB/s eta 0:00:18\n",
      "   -- ------------------------------------- 23.1/377.0 MB 21.8 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 24.1/377.0 MB 21.8 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 24.7/377.0 MB 21.1 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 26.4/377.0 MB 19.9 MB/s eta 0:00:18\n",
      "   -- ------------------------------------- 27.5/377.0 MB 20.5 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 28.6/377.0 MB 20.5 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 29.7/377.0 MB 19.9 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 30.5/377.0 MB 21.8 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 31.6/377.0 MB 25.2 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 32.2/377.0 MB 24.2 MB/s eta 0:00:15\n",
      "   --- ------------------------------------ 33.2/377.0 MB 22.6 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 33.8/377.0 MB 22.6 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 34.5/377.0 MB 21.8 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 35.4/377.0 MB 22.6 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 36.2/377.0 MB 21.9 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 37.2/377.0 MB 21.1 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 38.1/377.0 MB 21.1 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 38.7/377.0 MB 20.5 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 39.7/377.0 MB 20.5 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 40.5/377.0 MB 19.2 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 40.7/377.0 MB 19.2 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 41.9/377.0 MB 19.3 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 42.1/377.0 MB 18.2 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 43.3/377.0 MB 18.2 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 44.1/377.0 MB 19.3 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 44.8/377.0 MB 18.7 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 45.5/377.0 MB 18.2 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 46.4/377.0 MB 18.7 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 47.2/377.0 MB 18.2 MB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 47.9/377.0 MB 17.7 MB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 48.5/377.0 MB 17.2 MB/s eta 0:00:20\n",
      "   ----- ---------------------------------- 49.3/377.0 MB 17.7 MB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 50.1/377.0 MB 17.2 MB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 50.9/377.0 MB 17.7 MB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 51.6/377.0 MB 18.7 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 52.2/377.0 MB 17.7 MB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 53.0/377.0 MB 18.2 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 53.9/377.0 MB 18.7 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 54.8/377.0 MB 18.7 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 55.6/377.0 MB 18.7 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 56.3/377.0 MB 18.7 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 57.1/377.0 MB 18.7 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 57.8/377.0 MB 18.7 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 58.3/377.0 MB 18.2 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 59.2/377.0 MB 18.7 MB/s eta 0:00:17\n",
      "   ------ --------------------------------- 59.8/377.0 MB 18.7 MB/s eta 0:00:17\n",
      "   ------ --------------------------------- 60.7/377.0 MB 18.2 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 61.5/377.0 MB 18.7 MB/s eta 0:00:17\n",
      "   ------ --------------------------------- 62.3/377.0 MB 18.2 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 62.8/377.0 MB 18.2 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 63.6/377.0 MB 18.2 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 64.5/377.0 MB 18.2 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 65.3/377.0 MB 18.7 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 66.1/377.0 MB 18.2 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 67.0/377.0 MB 18.7 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 68.0/377.0 MB 19.3 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 68.7/377.0 MB 19.3 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 69.8/377.0 MB 19.9 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 70.7/377.0 MB 20.5 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 71.5/377.0 MB 20.5 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 72.5/377.0 MB 21.1 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 73.4/377.0 MB 21.9 MB/s eta 0:00:14\n",
      "   ------- -------------------------------- 74.2/377.0 MB 21.1 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 75.1/377.0 MB 21.8 MB/s eta 0:00:14\n",
      "   -------- ------------------------------- 76.3/377.0 MB 21.8 MB/s eta 0:00:14\n",
      "   -------- ------------------------------- 77.0/377.0 MB 21.9 MB/s eta 0:00:14\n",
      "   -------- ------------------------------- 77.6/377.0 MB 21.1 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 78.6/377.0 MB 20.5 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 79.6/377.0 MB 21.1 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 80.5/377.0 MB 21.1 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 81.4/377.0 MB 20.5 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 82.3/377.0 MB 21.1 MB/s eta 0:00:14\n",
      "   -------- ------------------------------- 83.3/377.0 MB 21.1 MB/s eta 0:00:14\n",
      "   -------- ------------------------------- 84.2/377.0 MB 21.1 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 85.2/377.0 MB 21.1 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 85.9/377.0 MB 21.1 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 86.8/377.0 MB 21.1 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 87.7/377.0 MB 21.8 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 88.6/377.0 MB 21.8 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 89.4/377.0 MB 21.1 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 90.3/377.0 MB 21.1 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 91.2/377.0 MB 21.8 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 91.9/377.0 MB 20.5 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 92.7/377.0 MB 20.5 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 93.7/377.0 MB 20.5 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 94.3/377.0 MB 19.8 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 95.5/377.0 MB 19.9 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 96.3/377.0 MB 19.9 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 97.3/377.0 MB 20.5 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 98.3/377.0 MB 20.5 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 99.1/377.0 MB 19.9 MB/s eta 0:00:14\n",
      "   ---------- ---------------------------- 100.0/377.0 MB 21.1 MB/s eta 0:00:14\n",
      "   ---------- ---------------------------- 100.8/377.0 MB 21.1 MB/s eta 0:00:14\n",
      "   ---------- ---------------------------- 101.6/377.0 MB 20.5 MB/s eta 0:00:14\n",
      "   ---------- ---------------------------- 102.3/377.0 MB 21.1 MB/s eta 0:00:14\n",
      "   ---------- ---------------------------- 103.2/377.0 MB 21.1 MB/s eta 0:00:13\n",
      "   ---------- ---------------------------- 104.0/377.0 MB 21.1 MB/s eta 0:00:13\n",
      "   ---------- ---------------------------- 104.6/377.0 MB 21.1 MB/s eta 0:00:13\n",
      "   ---------- ---------------------------- 105.4/377.0 MB 20.5 MB/s eta 0:00:14\n",
      "   ----------- --------------------------- 106.3/377.0 MB 19.9 MB/s eta 0:00:14\n",
      "   ----------- --------------------------- 107.2/377.0 MB 19.9 MB/s eta 0:00:14\n",
      "   ----------- --------------------------- 108.0/377.0 MB 19.8 MB/s eta 0:00:14\n",
      "   ----------- --------------------------- 108.6/377.0 MB 19.8 MB/s eta 0:00:14\n",
      "   ----------- --------------------------- 109.6/377.0 MB 19.3 MB/s eta 0:00:14\n",
      "   ----------- --------------------------- 110.5/377.0 MB 19.3 MB/s eta 0:00:14\n",
      "   ----------- --------------------------- 111.1/377.0 MB 19.3 MB/s eta 0:00:14\n",
      "   ----------- --------------------------- 111.8/377.0 MB 18.7 MB/s eta 0:00:15\n",
      "   ----------- --------------------------- 112.6/377.0 MB 18.7 MB/s eta 0:00:15\n",
      "   ----------- --------------------------- 113.5/377.0 MB 19.3 MB/s eta 0:00:14\n",
      "   ----------- --------------------------- 114.1/377.0 MB 18.2 MB/s eta 0:00:15\n",
      "   ----------- --------------------------- 114.7/377.0 MB 18.2 MB/s eta 0:00:15\n",
      "   ----------- --------------------------- 115.3/377.0 MB 18.2 MB/s eta 0:00:15\n",
      "   ----------- --------------------------- 115.9/377.0 MB 17.7 MB/s eta 0:00:15\n",
      "   ------------ -------------------------- 116.7/377.0 MB 17.7 MB/s eta 0:00:15\n",
      "   ------------ -------------------------- 117.5/377.0 MB 17.2 MB/s eta 0:00:16\n",
      "   ------------ -------------------------- 118.4/377.0 MB 17.3 MB/s eta 0:00:15\n",
      "   ------------ -------------------------- 119.3/377.0 MB 17.7 MB/s eta 0:00:15\n",
      "   ------------ -------------------------- 120.4/377.0 MB 18.2 MB/s eta 0:00:15\n",
      "   ------------ -------------------------- 121.2/377.0 MB 18.7 MB/s eta 0:00:14\n",
      "   ------------ -------------------------- 122.4/377.0 MB 19.8 MB/s eta 0:00:13\n",
      "   ------------ -------------------------- 123.4/377.0 MB 20.5 MB/s eta 0:00:13\n",
      "   ------------ -------------------------- 124.4/377.0 MB 21.1 MB/s eta 0:00:12\n",
      "   ------------ -------------------------- 125.6/377.0 MB 22.6 MB/s eta 0:00:12\n",
      "   ------------- ------------------------- 126.3/377.0 MB 22.6 MB/s eta 0:00:12\n",
      "   ------------- ------------------------- 127.6/377.0 MB 23.4 MB/s eta 0:00:11\n",
      "   ------------- ------------------------- 128.5/377.0 MB 24.2 MB/s eta 0:00:11\n",
      "   ------------- ------------------------- 129.2/377.0 MB 23.4 MB/s eta 0:00:11\n",
      "   ------------- ------------------------- 130.5/377.0 MB 24.2 MB/s eta 0:00:11\n",
      "   ------------- ------------------------- 131.5/377.0 MB 24.2 MB/s eta 0:00:11\n",
      "   ------------- ------------------------- 132.8/377.0 MB 25.2 MB/s eta 0:00:10\n",
      "   ------------- ------------------------- 133.7/377.0 MB 24.2 MB/s eta 0:00:11\n",
      "   ------------- ------------------------- 134.6/377.0 MB 24.2 MB/s eta 0:00:10\n",
      "   -------------- ------------------------ 135.6/377.0 MB 25.2 MB/s eta 0:00:10\n",
      "   -------------- ------------------------ 136.5/377.0 MB 23.4 MB/s eta 0:00:11\n",
      "   -------------- ------------------------ 137.6/377.0 MB 24.2 MB/s eta 0:00:10\n",
      "   -------------- ------------------------ 138.6/377.0 MB 25.2 MB/s eta 0:00:10\n",
      "   -------------- ------------------------ 139.8/377.0 MB 26.2 MB/s eta 0:00:10\n",
      "   -------------- ------------------------ 140.6/377.0 MB 24.2 MB/s eta 0:00:10\n",
      "   -------------- ------------------------ 141.7/377.0 MB 25.2 MB/s eta 0:00:10\n",
      "   -------------- ------------------------ 142.8/377.0 MB 24.2 MB/s eta 0:00:10\n",
      "   -------------- ------------------------ 143.6/377.0 MB 24.2 MB/s eta 0:00:10\n",
      "   -------------- ------------------------ 144.5/377.0 MB 24.3 MB/s eta 0:00:10\n",
      "   --------------- ----------------------- 145.6/377.0 MB 25.1 MB/s eta 0:00:10\n",
      "   --------------- ----------------------- 146.5/377.0 MB 25.1 MB/s eta 0:00:10\n",
      "   --------------- ----------------------- 147.3/377.0 MB 24.3 MB/s eta 0:00:10\n",
      "   --------------- ----------------------- 148.4/377.0 MB 23.4 MB/s eta 0:00:10\n",
      "   --------------- ----------------------- 149.0/377.0 MB 23.4 MB/s eta 0:00:10\n",
      "   --------------- ----------------------- 150.0/377.0 MB 22.6 MB/s eta 0:00:11\n",
      "   --------------- ----------------------- 151.0/377.0 MB 22.6 MB/s eta 0:00:11\n",
      "   --------------- ----------------------- 152.2/377.0 MB 23.4 MB/s eta 0:00:10\n",
      "   --------------- ----------------------- 153.3/377.0 MB 24.2 MB/s eta 0:00:10\n",
      "   --------------- ----------------------- 154.2/377.0 MB 24.2 MB/s eta 0:00:10\n",
      "   ---------------- ---------------------- 155.1/377.0 MB 23.4 MB/s eta 0:00:10\n",
      "   ---------------- ---------------------- 156.4/377.0 MB 25.2 MB/s eta 0:00:09\n",
      "   ---------------- ---------------------- 157.4/377.0 MB 25.1 MB/s eta 0:00:09\n",
      "   ---------------- ---------------------- 157.9/377.0 MB 22.5 MB/s eta 0:00:10\n",
      "   ---------------- ---------------------- 158.7/377.0 MB 22.6 MB/s eta 0:00:10\n",
      "   ---------------- ---------------------- 159.7/377.0 MB 24.2 MB/s eta 0:00:09\n",
      "   ---------------- ---------------------- 160.6/377.0 MB 23.4 MB/s eta 0:00:10\n",
      "   ---------------- ---------------------- 161.8/377.0 MB 23.4 MB/s eta 0:00:10\n",
      "   ---------------- ---------------------- 162.9/377.0 MB 23.4 MB/s eta 0:00:10\n",
      "   ---------------- ---------------------- 164.0/377.0 MB 23.4 MB/s eta 0:00:10\n",
      "   ----------------- --------------------- 165.2/377.0 MB 24.3 MB/s eta 0:00:09\n",
      "   ----------------- --------------------- 166.1/377.0 MB 24.2 MB/s eta 0:00:09\n",
      "   ----------------- --------------------- 166.9/377.0 MB 25.2 MB/s eta 0:00:09\n",
      "   ----------------- --------------------- 167.8/377.0 MB 23.4 MB/s eta 0:00:09\n",
      "   ----------------- --------------------- 168.9/377.0 MB 24.2 MB/s eta 0:00:09\n",
      "   ----------------- --------------------- 170.1/377.0 MB 25.2 MB/s eta 0:00:09\n",
      "   ----------------- --------------------- 171.0/377.0 MB 25.2 MB/s eta 0:00:09\n",
      "   ----------------- --------------------- 172.2/377.0 MB 26.2 MB/s eta 0:00:08\n",
      "   ----------------- --------------------- 173.3/377.0 MB 25.2 MB/s eta 0:00:09\n",
      "   ------------------ -------------------- 174.4/377.0 MB 26.2 MB/s eta 0:00:08\n",
      "   ------------------ -------------------- 175.2/377.0 MB 26.2 MB/s eta 0:00:08\n",
      "   ------------------ -------------------- 176.4/377.0 MB 25.2 MB/s eta 0:00:08\n",
      "   ------------------ -------------------- 177.4/377.0 MB 25.2 MB/s eta 0:00:08\n",
      "   ------------------ -------------------- 178.5/377.0 MB 26.2 MB/s eta 0:00:08\n",
      "   ------------------ -------------------- 179.9/377.0 MB 26.2 MB/s eta 0:00:08\n",
      "   ------------------ -------------------- 180.8/377.0 MB 27.3 MB/s eta 0:00:08\n",
      "   ------------------ -------------------- 181.9/377.0 MB 27.3 MB/s eta 0:00:08\n",
      "   ------------------ -------------------- 182.8/377.0 MB 26.2 MB/s eta 0:00:08\n",
      "   ------------------- ------------------- 184.0/377.0 MB 26.2 MB/s eta 0:00:08\n",
      "   ------------------- ------------------- 184.9/377.0 MB 26.2 MB/s eta 0:00:08\n",
      "   ------------------- ------------------- 186.2/377.0 MB 27.3 MB/s eta 0:00:07\n",
      "   ------------------- ------------------- 187.3/377.0 MB 27.3 MB/s eta 0:00:07\n",
      "   ------------------- ------------------- 188.3/377.0 MB 26.2 MB/s eta 0:00:08\n",
      "   ------------------- ------------------- 189.4/377.0 MB 26.2 MB/s eta 0:00:08\n",
      "   ------------------- ------------------- 190.8/377.0 MB 27.3 MB/s eta 0:00:07\n",
      "   ------------------- ------------------- 191.5/377.0 MB 26.2 MB/s eta 0:00:08\n",
      "   ------------------- ------------------- 192.4/377.0 MB 25.2 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 193.5/377.0 MB 26.2 MB/s eta 0:00:07\n",
      "   -------------------- ------------------ 194.7/377.0 MB 27.3 MB/s eta 0:00:07\n",
      "   -------------------- ------------------ 195.5/377.0 MB 26.2 MB/s eta 0:00:07\n",
      "   -------------------- ------------------ 196.4/377.0 MB 26.2 MB/s eta 0:00:07\n",
      "   -------------------- ------------------ 197.7/377.0 MB 26.2 MB/s eta 0:00:07\n",
      "   -------------------- ------------------ 198.8/377.0 MB 26.2 MB/s eta 0:00:07\n",
      "   -------------------- ------------------ 200.1/377.0 MB 26.2 MB/s eta 0:00:07\n",
      "   -------------------- ------------------ 201.0/377.0 MB 25.2 MB/s eta 0:00:07\n",
      "   -------------------- ------------------ 202.1/377.0 MB 26.2 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 203.4/377.0 MB 27.3 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 204.5/377.0 MB 27.3 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 205.5/377.0 MB 27.3 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 206.4/377.0 MB 26.2 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 207.3/377.0 MB 26.2 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 208.3/377.0 MB 25.2 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 209.4/377.0 MB 26.2 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 210.1/377.0 MB 24.3 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 211.1/377.0 MB 25.1 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 212.3/377.0 MB 25.1 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 213.4/377.0 MB 24.3 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 214.3/377.0 MB 23.4 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 215.5/377.0 MB 25.1 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 216.7/377.0 MB 24.3 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 217.6/377.0 MB 26.2 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 218.4/377.0 MB 24.2 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 219.3/377.0 MB 25.2 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 220.2/377.0 MB 25.1 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 221.3/377.0 MB 25.1 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 222.1/377.0 MB 24.2 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 223.0/377.0 MB 24.2 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 224.1/377.0 MB 24.2 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 225.1/377.0 MB 23.4 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 226.0/377.0 MB 24.2 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 227.2/377.0 MB 25.2 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 228.1/377.0 MB 22.6 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 229.4/377.0 MB 24.2 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 230.4/377.0 MB 25.2 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 231.6/377.0 MB 25.2 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 232.9/377.0 MB 27.3 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 234.0/377.0 MB 26.2 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 235.2/377.0 MB 27.3 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 236.2/377.0 MB 26.2 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 237.1/377.0 MB 26.2 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 238.0/377.0 MB 26.2 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 238.8/377.0 MB 26.2 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 240.1/377.0 MB 27.3 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 241.2/377.0 MB 26.2 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 242.7/377.0 MB 26.2 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 243.7/377.0 MB 26.2 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 245.0/377.0 MB 26.2 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 246.0/377.0 MB 25.2 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 247.0/377.0 MB 26.2 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 248.2/377.0 MB 26.2 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 249.4/377.0 MB 27.3 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 250.5/377.0 MB 27.3 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 251.5/377.0 MB 26.2 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 252.5/377.0 MB 26.2 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 253.7/377.0 MB 26.2 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 254.6/377.0 MB 26.2 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 255.6/377.0 MB 26.2 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 256.4/377.0 MB 25.1 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 257.3/377.0 MB 25.2 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 258.2/377.0 MB 24.2 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 259.4/377.0 MB 24.2 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 260.5/377.0 MB 24.3 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 261.7/377.0 MB 25.1 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 262.6/377.0 MB 24.2 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 263.6/377.0 MB 23.4 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 264.6/377.0 MB 24.2 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 265.7/377.0 MB 24.2 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 266.7/377.0 MB 25.2 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 267.4/377.0 MB 24.2 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 268.5/377.0 MB 25.1 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 269.7/377.0 MB 26.2 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 270.7/377.0 MB 25.1 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 271.9/377.0 MB 24.2 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 273.0/377.0 MB 26.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 273.9/377.0 MB 25.1 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 275.0/377.0 MB 26.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 275.9/377.0 MB 25.2 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 276.9/377.0 MB 25.1 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 278.1/377.0 MB 27.3 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 279.3/377.0 MB 26.2 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 280.7/377.0 MB 27.3 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 281.5/377.0 MB 26.2 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 282.6/377.0 MB 26.2 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 283.6/377.0 MB 26.2 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 284.4/377.0 MB 26.2 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 285.5/377.0 MB 26.2 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 286.9/377.0 MB 27.3 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 288.1/377.0 MB 27.3 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 289.0/377.0 MB 26.2 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 290.0/377.0 MB 26.2 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 290.9/377.0 MB 24.2 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 292.1/377.0 MB 26.2 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 293.0/377.0 MB 26.2 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 294.0/377.0 MB 26.2 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 295.2/377.0 MB 26.2 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 296.6/377.0 MB 26.2 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 297.3/377.0 MB 25.2 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 298.6/377.0 MB 25.2 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 300.0/377.0 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 301.1/377.0 MB 27.3 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 302.2/377.0 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 303.2/377.0 MB 27.3 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 304.3/377.0 MB 27.3 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 305.4/377.0 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 306.4/377.0 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 307.8/377.0 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 309.1/377.0 MB 28.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 310.3/377.0 MB 27.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 311.3/377.0 MB 27.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 312.5/377.0 MB 28.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 313.8/377.0 MB 28.5 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 314.7/377.0 MB 28.5 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 315.8/377.0 MB 27.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 316.9/377.0 MB 28.5 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 318.1/377.0 MB 28.5 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 319.0/377.0 MB 26.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 319.7/377.0 MB 26.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 321.0/377.0 MB 26.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 322.6/377.0 MB 26.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 323.3/377.0 MB 25.1 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 324.4/377.0 MB 25.1 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 325.6/377.0 MB 25.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 326.5/377.0 MB 24.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 327.4/377.0 MB 24.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 327.9/377.0 MB 22.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 329.0/377.0 MB 24.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 329.7/377.0 MB 22.5 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 331.0/377.0 MB 22.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 331.3/377.0 MB 22.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 332.3/377.0 MB 21.1 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 333.3/377.0 MB 21.9 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 334.7/377.0 MB 23.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 335.9/377.0 MB 23.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 337.1/377.0 MB 23.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 338.1/377.0 MB 25.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 339.4/377.0 MB 26.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 340.7/377.0 MB 27.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 341.4/377.0 MB 27.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 342.5/377.0 MB 28.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 343.6/377.0 MB 28.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 344.3/377.0 MB 26.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 345.4/377.0 MB 26.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 346.6/377.0 MB 26.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 347.6/377.0 MB 27.3 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 348.9/377.0 MB 26.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 349.9/377.0 MB 25.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 351.2/377.0 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 352.3/377.0 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 353.0/377.0 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 354.0/377.0 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 355.0/377.0 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 356.1/377.0 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 356.6/377.0 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 357.3/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 358.3/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 359.4/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 360.0/377.0 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 360.4/377.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 360.6/377.0 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 360.9/377.0 MB 19.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 361.8/377.0 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 362.6/377.0 MB 18.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 363.1/377.0 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 363.7/377.0 MB 17.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 364.5/377.0 MB 17.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 365.3/377.0 MB 16.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 366.2/377.0 MB 16.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 366.6/377.0 MB 16.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  367.5/377.0 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  368.3/377.0 MB 16.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  369.1/377.0 MB 16.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  370.2/377.0 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  371.3/377.0 MB 19.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  371.9/377.0 MB 19.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  373.1/377.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  374.1/377.0 MB 20.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  374.8/377.0 MB 21.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/377.0 MB 21.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  376.8/377.0 MB 21.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 377.0/377.0 MB 7.2 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 133.7/133.7 kB 8.2 MB/s eta 0:00:00\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.5/57.5 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading grpcio-1.62.1-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 1.1/3.8 MB 23.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.0/3.8 MB 25.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.9/3.8 MB 23.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.4/3.8 MB 19.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.8/3.8 MB 18.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 16.1 MB/s eta 0:00:00\n",
      "Downloading h5py-3.10.0-cp311-cp311-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.8/2.7 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.5/2.7 MB 15.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.2/2.7 MB 17.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 15.5 MB/s eta 0:00:00\n",
      "Downloading keras-3.1.1-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 0.5/1.1 MB 30.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 16.9 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.7/26.4 MB 21.1 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 1.7/26.4 MB 21.4 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 2.7/26.4 MB 21.5 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 3.7/26.4 MB 21.7 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 4.8/26.4 MB 21.6 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 5.8/26.4 MB 21.6 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 6.7/26.4 MB 21.5 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 7.8/26.4 MB 21.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 8.9/26.4 MB 21.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 9.8/26.4 MB 21.5 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 10.5/26.4 MB 21.8 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 11.4/26.4 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 12.4/26.4 MB 20.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 13.4/26.4 MB 20.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 14.5/26.4 MB 20.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 15.5/26.4 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 16.6/26.4 MB 21.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 17.7/26.4 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 18.7/26.4 MB 21.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 19.7/26.4 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 20.8/26.4 MB 21.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 21.7/26.4 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 22.5/26.4 MB 21.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 23.5/26.4 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 24.4/26.4 MB 21.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 25.4/26.4 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 14.5 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.3.2-cp311-cp311-win_amd64.whl (127 kB)\n",
      "   ---------------------------------------- 0.0/127.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 127.7/127.7 kB 7.3 MB/s eta 0:00:00\n",
      "Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "   ---------------------------------------- 0.0/65.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 65.5/65.5 kB ? eta 0:00:00\n",
      "Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 413.4/413.4 kB 26.9 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/5.5 MB 24.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.8/5.5 MB 23.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.9/5.5 MB 23.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 3.9/5.5 MB 22.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 4.9/5.5 MB 22.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.5 MB 21.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 19.5 MB/s eta 0:00:00\n",
      "Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 1.0/1.5 MB 21.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 18.9 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.11.0-cp311-cp311-win_amd64.whl (245 kB)\n",
      "   ---------------------------------------- 0.0/245.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 245.0/245.0 kB 14.7 MB/s eta 0:00:00\n",
      "Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "   ---------------------------------------- 0.0/240.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 240.7/240.7 kB 14.4 MB/s eta 0:00:00\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, ml-dtypes, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, rich, keras, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.9.0\n",
      "    Uninstalling h5py-3.9.0:\n",
      "      Successfully uninstalled h5py-3.9.0\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.5.4 google-pasta-0.2.0 grpcio-1.62.1 h5py-3.10.0 keras-3.1.1 libclang-18.1.1 ml-dtypes-0.3.2 namex-0.0.7 opt-einsum-3.3.0 optree-0.11.0 protobuf-4.25.3 rich-13.7.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-intel-2.16.1 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "434b8218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NHanamasagar\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pyod\\models\\ecod.py:25: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  return np.nan_to_num(skew_sp(X, axis=axis))\n",
      "C:\\Users\\NHanamasagar\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │            \u001b[38;5;34m90\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │            \u001b[38;5;34m90\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │            \u001b[38;5;34m90\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │            \u001b[38;5;34m50\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_17 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m12\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_18 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │            \u001b[38;5;34m15\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │            \u001b[38;5;34m54\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │            \u001b[38;5;34m90\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">491</span> (1.92 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m491\u001b[0m (1.92 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">491</span> (1.92 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m491\u001b[0m (1.92 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 126ms/step - loss: 5.7988 - val_loss: 1.3715\n",
      "Epoch 2/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 5.7837 - val_loss: 1.3680\n",
      "Epoch 3/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.7701 - val_loss: 1.3644\n",
      "Epoch 4/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.7553 - val_loss: 1.3608\n",
      "Epoch 5/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5.7417 - val_loss: 1.3573\n",
      "Epoch 6/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.7263 - val_loss: 1.3537\n",
      "Epoch 7/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.7125 - val_loss: 1.3502\n",
      "Epoch 8/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5.6957 - val_loss: 1.3466\n",
      "Epoch 9/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 5.6823 - val_loss: 1.3430\n",
      "Epoch 10/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.6663 - val_loss: 1.3393\n",
      "Epoch 11/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 5.6478 - val_loss: 1.3356\n",
      "Epoch 12/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.6376 - val_loss: 1.3319\n",
      "Epoch 13/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 5.6188 - val_loss: 1.3281\n",
      "Epoch 14/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.6090 - val_loss: 1.3243\n",
      "Epoch 15/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.5889 - val_loss: 1.3204\n",
      "Epoch 16/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 5.5730 - val_loss: 1.3165\n",
      "Epoch 17/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.5663 - val_loss: 1.3126\n",
      "Epoch 18/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 5.5480 - val_loss: 1.3086\n",
      "Epoch 19/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.5191 - val_loss: 1.3045\n",
      "Epoch 20/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.5072 - val_loss: 1.3004\n",
      "Epoch 21/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 5.4851 - val_loss: 1.2962\n",
      "Epoch 22/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.4810 - val_loss: 1.2920\n",
      "Epoch 23/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 5.4472 - val_loss: 1.2877\n",
      "Epoch 24/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.4537 - val_loss: 1.2834\n",
      "Epoch 25/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.4119 - val_loss: 1.2790\n",
      "Epoch 26/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 5.4106 - val_loss: 1.2746\n",
      "Epoch 27/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.3733 - val_loss: 1.2700\n",
      "Epoch 28/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 5.3739 - val_loss: 1.2655\n",
      "Epoch 29/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5.3434 - val_loss: 1.2610\n",
      "Epoch 30/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 5.3291 - val_loss: 1.2564\n",
      "Epoch 31/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5.3125 - val_loss: 1.2517\n",
      "Epoch 32/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 5.2925 - val_loss: 1.2470\n",
      "Epoch 33/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5.2800 - val_loss: 1.2422\n",
      "Epoch 34/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.2699 - val_loss: 1.2374\n",
      "Epoch 35/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 5.2337 - val_loss: 1.2326\n",
      "Epoch 36/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.2324 - val_loss: 1.2279\n",
      "Epoch 37/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5.1962 - val_loss: 1.2230\n",
      "Epoch 38/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.1743 - val_loss: 1.2180\n",
      "Epoch 39/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.1572 - val_loss: 1.2129\n",
      "Epoch 40/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 5.1425 - val_loss: 1.2078\n",
      "Epoch 41/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.1026 - val_loss: 1.2025\n",
      "Epoch 42/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 5.1084 - val_loss: 1.1973\n",
      "Epoch 43/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5.0663 - val_loss: 1.1921\n",
      "Epoch 44/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 5.0416 - val_loss: 1.1868\n",
      "Epoch 45/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.0477 - val_loss: 1.1815\n",
      "Epoch 46/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 5.0030 - val_loss: 1.1763\n",
      "Epoch 47/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.0273 - val_loss: 1.1712\n",
      "Epoch 48/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9776 - val_loss: 1.1659\n",
      "Epoch 49/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.9549 - val_loss: 1.1607\n",
      "Epoch 50/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 4.9055 - val_loss: 1.1554\n",
      "Epoch 51/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.8988 - val_loss: 1.1500\n",
      "Epoch 52/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 4.9076 - val_loss: 1.1446\n",
      "Epoch 53/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8977 - val_loss: 1.1392\n",
      "Epoch 54/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.8432 - val_loss: 1.1338\n",
      "Epoch 55/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 4.7711 - val_loss: 1.1283\n",
      "Epoch 56/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7794 - val_loss: 1.1227\n",
      "Epoch 57/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8110 - val_loss: 1.1171\n",
      "Epoch 58/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7692 - val_loss: 1.1116\n",
      "Epoch 59/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.7226 - val_loss: 1.1061\n",
      "Epoch 60/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 4.7455 - val_loss: 1.1007\n",
      "Epoch 61/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6749 - val_loss: 1.0951\n",
      "Epoch 62/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.6706 - val_loss: 1.0896\n",
      "Epoch 63/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.6154 - val_loss: 1.0839\n",
      "Epoch 64/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5741 - val_loss: 1.0782\n",
      "Epoch 65/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 4.6120 - val_loss: 1.0724\n",
      "Epoch 66/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5656 - val_loss: 1.0668\n",
      "Epoch 67/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 4.5314 - val_loss: 1.0611\n",
      "Epoch 68/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.4952 - val_loss: 1.0555\n",
      "Epoch 69/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5207 - val_loss: 1.0499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.5084 - val_loss: 1.0445\n",
      "Epoch 71/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.5277 - val_loss: 1.0393\n",
      "Epoch 72/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 4.4795 - val_loss: 1.0342\n",
      "Epoch 73/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.4064 - val_loss: 1.0290\n",
      "Epoch 74/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.4521 - val_loss: 1.0238\n",
      "Epoch 75/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 4.3788 - val_loss: 1.0184\n",
      "Epoch 76/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.3306 - val_loss: 1.0130\n",
      "Epoch 77/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3600 - val_loss: 1.0075\n",
      "Epoch 78/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.3065 - val_loss: 1.0021\n",
      "Epoch 79/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 4.3045 - val_loss: 0.9967\n",
      "Epoch 80/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.2911 - val_loss: 0.9914\n",
      "Epoch 81/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.2511 - val_loss: 0.9861\n",
      "Epoch 82/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.2457 - val_loss: 0.9808\n",
      "Epoch 83/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.2681 - val_loss: 0.9755\n",
      "Epoch 84/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.1852 - val_loss: 0.9703\n",
      "Epoch 85/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.2103 - val_loss: 0.9652\n",
      "Epoch 86/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.0905 - val_loss: 0.9601\n",
      "Epoch 87/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.1159 - val_loss: 0.9550\n",
      "Epoch 88/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.2228 - val_loss: 0.9501\n",
      "Epoch 89/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 4.0734 - val_loss: 0.9451\n",
      "Epoch 90/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.0674 - val_loss: 0.9401\n",
      "Epoch 91/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 4.0430 - val_loss: 0.9351\n",
      "Epoch 92/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.0350 - val_loss: 0.9301\n",
      "Epoch 93/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.0377 - val_loss: 0.9251\n",
      "Epoch 94/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.0668 - val_loss: 0.9203\n",
      "Epoch 95/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.0073 - val_loss: 0.9155\n",
      "Epoch 96/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.9526 - val_loss: 0.9107\n",
      "Epoch 97/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.8645 - val_loss: 0.9058\n",
      "Epoch 98/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.9747 - val_loss: 0.9010\n",
      "Epoch 99/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3.9508 - val_loss: 0.8963\n",
      "Epoch 100/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.8002 - val_loss: 0.8915\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000024D699C7EC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 70ms/stepWARNING:tensorflow:6 out of the last 10 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000024D699C7EC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Isolation Forest:\n",
      "['OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK'\n",
      " 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK'\n",
      " 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK'\n",
      " 'OK' 'OK' 'OK' 'OK' 'OK']\n",
      "ECOD:\n",
      "['ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT']\n",
      "AutoEncoder:\n",
      "['ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT']\n",
      "LOF:\n",
      "['ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NHanamasagar\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pyod\\models\\ecod.py:25: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  return np.nan_to_num(skew_sp(X, axis=axis))\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from pyod.models.ecod import ECOD\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from pyod.models.lof import LOF\n",
    "\n",
    "# Load JSON data from file or API request\n",
    "with open('transaction_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Preprocess JSON data and extract relevant features\n",
    "def preprocess_data(json_data):\n",
    "    # Extract relevant features from the JSON data\n",
    "    # Example: transaction amount, frequency, location, etc.\n",
    "    processed_data = []\n",
    "    for entry in json_data:\n",
    "        # Extract relevant features and create a feature vector\n",
    "        feature_vector = [\n",
    "            float(json_data['transactionAmount']),\n",
    "            int(json_data['dateTimeTransaction']),\n",
    "            int(json_data['merchantCategoryCode']),\n",
    "            int(json_data['cardAcceptorId']),\n",
    "            float(json_data['cardBalance']),\n",
    "            float(json_data['channel'])\n",
    "            # Add more features as needed\n",
    "        ]\n",
    "        processed_data.append(feature_vector)\n",
    "    return np.array(processed_data)\n",
    "\n",
    "# Convert JSON data to feature matrix\n",
    "X = preprocess_data(data)\n",
    "\n",
    "# Initialize and fit Isolation Forest model\n",
    "iforest_model = IsolationForest(contamination=0.01, random_state=42)\n",
    "iforest_model.fit(X)\n",
    "\n",
    "# Initialize and fit ECOD model\n",
    "ecod_model = ECOD(contamination=0.01)\n",
    "ecod_model.fit(X)\n",
    "\n",
    "# Initialize and fit AutoEncoder model\n",
    "autoencoder_model = AutoEncoder(hidden_neurons=[9, 5, 2, 5, 9], contamination=0.01)\n",
    "autoencoder_model.fit(X)\n",
    "\n",
    "\n",
    "# Initialize and fit LOF model\n",
    "lof_model = LOF(contamination=0.01)\n",
    "lof_model.fit(X)\n",
    "\n",
    "# Predict outliers/anomalies using the trained models\n",
    "iforest_predictions = iforest_model.predict(X)\n",
    "ecod_predictions = ecod_model.predict(X)\n",
    "autoencoder_predictions = autoencoder_model.predict(X)\n",
    "lof_predictions = lof_model.predict(X)\n",
    "\n",
    "# Convert predictions to binary labels (1 for inliers, -1 for outliers)\n",
    "iforest_labels = np.where(iforest_predictions == 1, \"OK\", \"ALERT\")\n",
    "ecod_labels = np.where(ecod_predictions == 1, \"OK\", \"ALERT\")\n",
    "autoencoder_labels = np.where(autoencoder_predictions == 1, \"OK\", \"ALERT\")\n",
    "lof_labels = np.where(lof_predictions == 1, \"OK\", \"ALERT\")\n",
    "\n",
    "# Output detection results\n",
    "print(\"Isolation Forest:\")\n",
    "print(iforest_labels)\n",
    "\n",
    "print(\"ECOD:\")\n",
    "print(ecod_labels)\n",
    "\n",
    "print(\"AutoEncoder:\")\n",
    "print(autoencoder_labels)\n",
    "\n",
    "print(\"LOF:\")\n",
    "print(lof_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fa8917e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 0 3 0 0 5 1 5 0 2 1 5 4 4 2 2 4 3 1 5 2 0 2 5 4 2 3 0 0 5 1 1 1 2 0 3 4\n",
      " 3 3 0 5 5 4 4 4 4 4 5 0 2 4 2 5 3 1 4 1 5 4 4 1 5 5 1 4 0 1 2 4 3 2 1 1 2\n",
      " 5 5 0 0 3 5 0 1 3 2 0 4 3 3 2 3 3 5 3 0 2 3 5 3 5 3 2 3 5 2 4 0 1 4 3 3 4\n",
      " 4 3 5 2 1 1 1 4 0 3 0 4 1 0 1 3 0 3 5 0 2 2 1 4 0 0 5 2 1 2 2 4 1 1 5 5 2\n",
      " 0 5 2 4 2 0 3 3 2 4 0 5 2 0 4 3 4 3 3 0 5 2 3 3 0 0 1 1 4 3 2 4 4 3 0 2 0\n",
      " 0 3 4 5 0 2 0 4 1 0 3 4 3 0 5 1 1 2 3 1 5 3 2 4 4 0 0 2 4 3 2 0 2 1 1 3 4\n",
      " 4 3 5 5 4 0 3 4 4 3 4 0 0 4 1 2 4 0 2 1 2 0 4 1 5 3 4 0 4 2 1 4 5 1 2 5 0\n",
      " 2 4 4 0 1 2 3 4 5 4 1 2 3 4 4 3 4 4 0 4 4 0 1 3 4 2 1 4 3 1 5 4 0 2 5 4 5\n",
      " 0 0 3 3 5 2 0 1 5 4 5 5 2 1 2 3 1 4 3 1 0 1 4 1 1 0 3 3 3 4 2 1 0 3 4 1 2\n",
      " 5 0 3 5 5 3 1 1 5 4 0 5 1 4 3 0 0 1 1 4 3 2 5 2 4 3 3 0 4 1 4 2 4 0 5 1 3\n",
      " 4 5 0 0 3 5 5 5 0 1 4 4 4 2 5 4 3 0 3 4 5 2 0 1 0 5 5 3 1 5 4 3 4 5 1 2 0\n",
      " 5 2 0 0 3 2 0 5 1 2 3 3 4 1 0 3 3 5 2 2 1 4 1 4 3 1 4 5 2 0 4 5 2 0 4 1 1\n",
      " 5 4 3 2 5 4 1 4 2 0 2 0 5 0 4 0 4 1 5 5 3 4 0 5 4 3 4 3 1 5 1 2 0 5 0 0 1\n",
      " 1 3 1 4 0 5 5 5 2 1 5 2 4 3 0 1 5 1 1 0 4 2 0 2 4 5 2 2 5 5 1 2 0 4 1 2 2\n",
      " 0 5 5 2 2 1 3 0 5 2 4 4 0 1 3 1 3 1 4 5 1 2 1 1 3 3 2 1 4 4 2 4 1 0 2 0 2\n",
      " 4 0 1 1 4 3 0 2 2 3 5 3 1 5 2 2 2 5 5 5 3 3 1 5 5 2 3 0 4 4 5 2 4 1 2 5 5\n",
      " 1 0 4 1 1 4 2 4 1 5 1 4 3 5 5 3 1 4 0 1 3 1 2 1 4 0 3 5 5 3 0 4 4 4 2 5 2\n",
      " 1 4 0 2 2 1 1 3 4 0 3 0 2 4 1 0 5 1 3 1 3 0 0 5 1 0 5 4 0 2 1 3 0 5 2 0 1\n",
      " 1 0 3 2 2 2 5 1 1 4 1 2 2 1 0 1 4 1 5 3 0 4 5 5 2 5 3 4 3 3 2 3 1 0 1 2 3\n",
      " 3 5 0 5 0 2 5 4 4 2 2 3 4 4 0 5 1 3 2 2 4 2 4 2 5 2 3 5 5 3 3 3 1 5 2 0 2\n",
      " 5 0 3 0 1 5 4 1 2 3 1 0 2 2 2 2 4 4 0 3 4 4 2 3 4 5 3 4 4 2 4 3 5 3 4 1 4\n",
      " 1 4 4 0 4 3 1 4 1 2 1 2 1 2 1 3 3 1 5 5 5 0 4 3 1 4 2 2 3 1 4 1 3 2 1 5 0\n",
      " 3 3 1 5 0 1 2 2 5 3 0 0 0 4 2 3 5 0 3 1 5 1 3 5 2 1 5 5 4 0 0 5 0 4 5 3 0\n",
      " 3 4 2 2 2 3 2 4 3 1 4 4 3 0 1 2 3 3 1 0 0 0 4 2 3 0 3 3 1 4 3 1 4 1 4 0 1\n",
      " 1 4 0 2 4 1 3 0 1 1 4 1 0 2 4 1 5 3 1 4 1 2 2 4 0 3 0 5 5 5 2 0 3 0 1 3 1\n",
      " 0 5 2 1 0 0 2 1 5 4 5 0 0 5 4 2 2 4 5 5 1 1 3 1 4 5 4 2 0 0 4 5 4 5 5 0 3\n",
      " 4 1 4 1 2 0 1 0 1 2 1 0 1 1 4 3 3 3 2 3 2 0 4 5 1 4 5 0 1 0 0 5 1 5 3 3 1\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from pyod.models.ecod import ECOD\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from pyod.models.lof import LOF\n",
    "\n",
    "# Load JSON data from file or API request\n",
    "df = pd.read_csv('MOCK_DATA.csv')\n",
    "df.dropna(inplace=True)\n",
    "#print(df)\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_data = label_encoder.fit_transform(df['channel'])\n",
    "print(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56e0ffcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NHanamasagar\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pyod\\models\\ecod.py:25: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  return np.nan_to_num(skew_sp(X, axis=axis))\n",
      "C:\\Users\\NHanamasagar\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_32 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m42\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_28 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_33 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m42\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_29 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_34 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m42\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_30 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_35 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m21\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_31 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m4\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_32 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │             \u001b[38;5;34m6\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_33 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_38 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m24\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_34 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_39 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m42\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">223</span> (892.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m223\u001b[0m (892.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">223</span> (892.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m223\u001b[0m (892.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 190ms/step - loss: 3.9494 - val_loss: 0.9981\n",
      "Epoch 2/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.9420 - val_loss: 0.9963\n",
      "Epoch 3/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 3.9349 - val_loss: 0.9945\n",
      "Epoch 4/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 3.9278 - val_loss: 0.9927\n",
      "Epoch 5/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 3.9208 - val_loss: 0.9910\n",
      "Epoch 6/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.9138 - val_loss: 0.9892\n",
      "Epoch 7/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 3.9067 - val_loss: 0.9874\n",
      "Epoch 8/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 3.8997 - val_loss: 0.9856\n",
      "Epoch 9/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.8927 - val_loss: 0.9839\n",
      "Epoch 10/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 3.8857 - val_loss: 0.9821\n",
      "Epoch 11/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.8787 - val_loss: 0.9803\n",
      "Epoch 12/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.8717 - val_loss: 0.9786\n",
      "Epoch 13/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.8648 - val_loss: 0.9768\n",
      "Epoch 14/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 3.8578 - val_loss: 0.9750\n",
      "Epoch 15/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 3.8508 - val_loss: 0.9733\n",
      "Epoch 16/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.8439 - val_loss: 0.9715\n",
      "Epoch 17/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 3.8369 - val_loss: 0.9698\n",
      "Epoch 18/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.8300 - val_loss: 0.9680\n",
      "Epoch 19/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.8231 - val_loss: 0.9662\n",
      "Epoch 20/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 3.8162 - val_loss: 0.9645\n",
      "Epoch 21/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.8092 - val_loss: 0.9627\n",
      "Epoch 22/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.8023 - val_loss: 0.9610\n",
      "Epoch 23/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.7954 - val_loss: 0.9593\n",
      "Epoch 24/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.7885 - val_loss: 0.9575\n",
      "Epoch 25/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.7817 - val_loss: 0.9558\n",
      "Epoch 26/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.7748 - val_loss: 0.9540\n",
      "Epoch 27/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.7679 - val_loss: 0.9523\n",
      "Epoch 28/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.7611 - val_loss: 0.9506\n",
      "Epoch 29/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 3.7542 - val_loss: 0.9488\n",
      "Epoch 30/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.7474 - val_loss: 0.9471\n",
      "Epoch 31/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.7406 - val_loss: 0.9454\n",
      "Epoch 32/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 3.7338 - val_loss: 0.9437\n",
      "Epoch 33/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 3.7269 - val_loss: 0.9419\n",
      "Epoch 34/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 3.7201 - val_loss: 0.9402\n",
      "Epoch 35/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.7134 - val_loss: 0.9385\n",
      "Epoch 36/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.7066 - val_loss: 0.9368\n",
      "Epoch 37/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.6998 - val_loss: 0.9351\n",
      "Epoch 38/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 3.6930 - val_loss: 0.9334\n",
      "Epoch 39/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.6863 - val_loss: 0.9317\n",
      "Epoch 40/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.6795 - val_loss: 0.9300\n",
      "Epoch 41/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 3.6728 - val_loss: 0.9283\n",
      "Epoch 42/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 3.6661 - val_loss: 0.9266\n",
      "Epoch 43/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 3.6594 - val_loss: 0.9249\n",
      "Epoch 44/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 3.6527 - val_loss: 0.9232\n",
      "Epoch 45/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 3.6460 - val_loss: 0.9215\n",
      "Epoch 46/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.6393 - val_loss: 0.9198\n",
      "Epoch 47/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 3.6326 - val_loss: 0.9181\n",
      "Epoch 48/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 3.6259 - val_loss: 0.9164\n",
      "Epoch 49/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.6193 - val_loss: 0.9147\n",
      "Epoch 50/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.6126 - val_loss: 0.9130\n",
      "Epoch 51/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.6060 - val_loss: 0.9114\n",
      "Epoch 52/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.5994 - val_loss: 0.9097\n",
      "Epoch 53/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.5927 - val_loss: 0.9080\n",
      "Epoch 54/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 3.5861 - val_loss: 0.9063\n",
      "Epoch 55/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.5795 - val_loss: 0.9047\n",
      "Epoch 56/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.5729 - val_loss: 0.9030\n",
      "Epoch 57/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.5664 - val_loss: 0.9013\n",
      "Epoch 58/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.5598 - val_loss: 0.8997\n",
      "Epoch 59/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 3.5532 - val_loss: 0.8980\n",
      "Epoch 60/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 3.5467 - val_loss: 0.8964\n",
      "Epoch 61/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 3.5401 - val_loss: 0.8947\n",
      "Epoch 62/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.5336 - val_loss: 0.8931\n",
      "Epoch 63/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.5271 - val_loss: 0.8914\n",
      "Epoch 64/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 3.5206 - val_loss: 0.8898\n",
      "Epoch 65/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.5141 - val_loss: 0.8881\n",
      "Epoch 66/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 3.5076 - val_loss: 0.8865\n",
      "Epoch 67/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.5011 - val_loss: 0.8848\n",
      "Epoch 68/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.4946 - val_loss: 0.8832\n",
      "Epoch 69/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.4882 - val_loss: 0.8816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 3.4817 - val_loss: 0.8799\n",
      "Epoch 71/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 3.4753 - val_loss: 0.8783\n",
      "Epoch 72/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.4688 - val_loss: 0.8767\n",
      "Epoch 73/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.4624 - val_loss: 0.8751\n",
      "Epoch 74/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 3.4560 - val_loss: 0.8734\n",
      "Epoch 75/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.4496 - val_loss: 0.8718\n",
      "Epoch 76/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 3.4432 - val_loss: 0.8702\n",
      "Epoch 77/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.4368 - val_loss: 0.8686\n",
      "Epoch 78/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.4304 - val_loss: 0.8670\n",
      "Epoch 79/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 3.4241 - val_loss: 0.8654\n",
      "Epoch 80/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 3.4177 - val_loss: 0.8638\n",
      "Epoch 81/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.4114 - val_loss: 0.8622\n",
      "Epoch 82/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.4050 - val_loss: 0.8606\n",
      "Epoch 83/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.3987 - val_loss: 0.8590\n",
      "Epoch 84/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 3.3924 - val_loss: 0.8574\n",
      "Epoch 85/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 3.3861 - val_loss: 0.8558\n",
      "Epoch 86/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.3798 - val_loss: 0.8542\n",
      "Epoch 87/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 3.3735 - val_loss: 0.8526\n",
      "Epoch 88/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 3.3672 - val_loss: 0.8510\n",
      "Epoch 89/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 3.3610 - val_loss: 0.8494\n",
      "Epoch 90/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.3547 - val_loss: 0.8478\n",
      "Epoch 91/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 3.3485 - val_loss: 0.8463\n",
      "Epoch 92/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.3422 - val_loss: 0.8447\n",
      "Epoch 93/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.3360 - val_loss: 0.8431\n",
      "Epoch 94/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 3.3298 - val_loss: 0.8415\n",
      "Epoch 95/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.3236 - val_loss: 0.8400\n",
      "Epoch 96/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 3.3174 - val_loss: 0.8384\n",
      "Epoch 97/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.3112 - val_loss: 0.8368\n",
      "Epoch 98/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.3050 - val_loss: 0.8353\n",
      "Epoch 99/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 3.2989 - val_loss: 0.8337\n",
      "Epoch 100/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.2927 - val_loss: 0.8322\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "Isolation Forest:\n",
      "['OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK'\n",
      " 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK'\n",
      " 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK' 'OK'\n",
      " 'OK' 'OK' 'OK' 'OK' 'OK']\n",
      "ECOD:\n",
      "['ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT']\n",
      "AutoEncoder:\n",
      "['ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT']\n",
      "LOF:\n",
      "['ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT' 'ALERT'\n",
      " 'ALERT' 'ALERT']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NHanamasagar\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pyod\\models\\ecod.py:25: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  return np.nan_to_num(skew_sp(X, axis=axis))\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pyod.models.ecod import ECOD\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from pyod.models.lof import LOF\n",
    "\n",
    "# Load JSON data from file or API request\n",
    "with open('transaction_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Preprocess JSON data and extract relevant features\n",
    "def preprocess_data(json_data):\n",
    "    # Extract relevant features from the JSON data\n",
    "    # Example: transaction amount, frequency, location, etc.\n",
    "    processed_data = []\n",
    "    for entry in json_data:\n",
    "        # Extract relevant features and create a feature vector\n",
    "        label_encoder = LabelEncoder()\n",
    "        #encoded_data = label_encoder.fit_transform(json_data['channel'])\n",
    "        encoded_channel = label_encoder.fit_transform([json_data['channel']])[0]\n",
    "        feature_vector = [\n",
    "            float(json_data['transactionAmount']),\n",
    "            int(json_data['dateTimeTransaction']),\n",
    "            int(json_data['merchantCategoryCode']),\n",
    "            int(json_data['cardAcceptorId']),\n",
    "            float(json_data['cardBalance']),\n",
    "            encoded_channel\n",
    "            # Add more features as needed\n",
    "        ]\n",
    "        processed_data.append(feature_vector)\n",
    "    return np.array(processed_data)\n",
    "\n",
    "# Convert JSON data to feature matrix\n",
    "X = preprocess_data(data)\n",
    "\n",
    "# Initialize and fit Isolation Forest model\n",
    "iforest_model = IsolationForest(contamination=0.01, random_state=42)\n",
    "iforest_model.fit(X)\n",
    "\n",
    "# Initialize and fit ECOD model\n",
    "ecod_model = ECOD(contamination=0.01)\n",
    "ecod_model.fit(X)\n",
    "\n",
    "# Initialize and fit AutoEncoder model\n",
    "autoencoder_model = AutoEncoder(hidden_neurons=[6, 3, 1, 3, 6], contamination=0.01)\n",
    "autoencoder_model.fit(X)\n",
    "\n",
    "\n",
    "# Initialize and fit LOF model\n",
    "lof_model = LOF(contamination=0.01)\n",
    "lof_model.fit(X)\n",
    "\n",
    "# Predict outliers/anomalies using the trained models\n",
    "iforest_predictions = iforest_model.predict(X)\n",
    "ecod_predictions = ecod_model.predict(X)\n",
    "autoencoder_predictions = autoencoder_model.predict(X)\n",
    "lof_predictions = lof_model.predict(X)\n",
    "\n",
    "# Convert predictions to binary labels (1 for inliers, -1 for outliers)\n",
    "iforest_labels = np.where(iforest_predictions == 1, \"OK\", \"ALERT\")\n",
    "ecod_labels = np.where(ecod_predictions == 1, \"OK\", \"ALERT\")\n",
    "autoencoder_labels = np.where(autoencoder_predictions == 1, \"OK\", \"ALERT\")\n",
    "lof_labels = np.where(lof_predictions == 1, \"OK\", \"ALERT\")\n",
    "\n",
    "# Output detection results\n",
    "print(\"Isolation Forest:\")\n",
    "print(iforest_labels)\n",
    "\n",
    "print(\"ECOD:\")\n",
    "print(ecod_labels)\n",
    "\n",
    "print(\"AutoEncoder:\")\n",
    "print(autoencoder_labels)\n",
    "\n",
    "print(\"LOF:\")\n",
    "print(lof_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7398a35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NHanamasagar\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_80 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_70 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_81 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_82 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_72 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_83 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_73 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_84 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_85 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_86 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_76 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_87 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_80 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m42\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_70 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_81 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m42\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_71 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_82 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │            \u001b[38;5;34m35\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_72 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_83 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m18\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_73 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_84 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m4\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_74 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_85 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │             \u001b[38;5;34m6\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_75 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_86 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │            \u001b[38;5;34m20\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_76 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_87 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m36\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">203</span> (812.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m203\u001b[0m (812.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">203</span> (812.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m203\u001b[0m (812.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 24.6058 - val_loss: 13.7937\n",
      "Epoch 2/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 18.1930 - val_loss: 11.2387\n",
      "Epoch 3/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.6537 - val_loss: 9.6529\n",
      "Epoch 4/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.3881 - val_loss: 8.5631\n",
      "Epoch 5/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.0185 - val_loss: 7.7442\n",
      "Epoch 6/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.9547 - val_loss: 7.0787\n",
      "Epoch 7/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.0960 - val_loss: 6.5316\n",
      "Epoch 8/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1548 - val_loss: 6.0698\n",
      "Epoch 9/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7600 - val_loss: 5.6668\n",
      "Epoch 10/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0073 - val_loss: 5.3198\n",
      "Epoch 11/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.5831 - val_loss: 5.0067\n",
      "Epoch 12/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1663 - val_loss: 4.7324\n",
      "Epoch 13/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9635 - val_loss: 4.4818\n",
      "Epoch 14/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3955 - val_loss: 4.2629\n",
      "Epoch 15/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1107 - val_loss: 4.0629\n",
      "Epoch 16/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0296 - val_loss: 3.8794\n",
      "Epoch 17/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7607 - val_loss: 3.7146\n",
      "Epoch 18/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4260 - val_loss: 3.5664\n",
      "Epoch 19/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3010 - val_loss: 3.4281\n",
      "Epoch 20/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1052 - val_loss: 3.3021\n",
      "Epoch 21/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9712 - val_loss: 3.1848\n",
      "Epoch 22/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7894 - val_loss: 3.0768\n",
      "Epoch 23/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.6750 - val_loss: 2.9761\n",
      "Epoch 24/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5647 - val_loss: 2.8828\n",
      "Epoch 25/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4415 - val_loss: 2.7961\n",
      "Epoch 26/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3211 - val_loss: 2.7145\n",
      "Epoch 27/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2543 - val_loss: 2.6375\n",
      "Epoch 28/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1151 - val_loss: 2.5658\n",
      "Epoch 29/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0489 - val_loss: 2.4979\n",
      "Epoch 30/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9380 - val_loss: 2.4341\n",
      "Epoch 31/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8831 - val_loss: 2.3736\n",
      "Epoch 32/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8023 - val_loss: 2.3162\n",
      "Epoch 33/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7236 - val_loss: 2.2618\n",
      "Epoch 34/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6476 - val_loss: 2.2099\n",
      "Epoch 35/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5765 - val_loss: 2.1608\n",
      "Epoch 36/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5392 - val_loss: 2.1140\n",
      "Epoch 37/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4673 - val_loss: 2.0692\n",
      "Epoch 38/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4181 - val_loss: 2.0262\n",
      "Epoch 39/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3587 - val_loss: 1.9852\n",
      "Epoch 40/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3076 - val_loss: 1.9461\n",
      "Epoch 41/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2478 - val_loss: 1.9085\n",
      "Epoch 42/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1954 - val_loss: 1.8725\n",
      "Epoch 43/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1694 - val_loss: 1.8378\n",
      "Epoch 44/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1232 - val_loss: 1.8045\n",
      "Epoch 45/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0881 - val_loss: 1.7725\n",
      "Epoch 46/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0467 - val_loss: 1.7418\n",
      "Epoch 47/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9898 - val_loss: 1.7122\n",
      "Epoch 48/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9515 - val_loss: 1.6837\n",
      "Epoch 49/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9328 - val_loss: 1.6562\n",
      "Epoch 50/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8846 - val_loss: 1.6298\n",
      "Epoch 51/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8511 - val_loss: 1.6043\n",
      "Epoch 52/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8246 - val_loss: 1.5796\n",
      "Epoch 53/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7805 - val_loss: 1.5559\n",
      "Epoch 54/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7805 - val_loss: 1.5329\n",
      "Epoch 55/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7375 - val_loss: 1.5107\n",
      "Epoch 56/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7085 - val_loss: 1.4893\n",
      "Epoch 57/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6924 - val_loss: 1.4685\n",
      "Epoch 58/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6607 - val_loss: 1.4484\n",
      "Epoch 59/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6114 - val_loss: 1.4290\n",
      "Epoch 60/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6045 - val_loss: 1.4103\n",
      "Epoch 61/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5893 - val_loss: 1.3921\n",
      "Epoch 62/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5645 - val_loss: 1.3745\n",
      "Epoch 63/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5357 - val_loss: 1.3575\n",
      "Epoch 64/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5262 - val_loss: 1.3409\n",
      "Epoch 65/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4869 - val_loss: 1.3250\n",
      "Epoch 66/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4704 - val_loss: 1.3094\n",
      "Epoch 67/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4617 - val_loss: 1.2944\n",
      "Epoch 68/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4289 - val_loss: 1.2798\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4239 - val_loss: 1.2657\n",
      "Epoch 70/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4007 - val_loss: 1.2520\n",
      "Epoch 71/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3961 - val_loss: 1.2387\n",
      "Epoch 72/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3565 - val_loss: 1.2258\n",
      "Epoch 73/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3565 - val_loss: 1.2133\n",
      "Epoch 74/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3295 - val_loss: 1.2011\n",
      "Epoch 75/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3129 - val_loss: 1.1893\n",
      "Epoch 76/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2992 - val_loss: 1.1778\n",
      "Epoch 77/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2743 - val_loss: 1.1666\n",
      "Epoch 78/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2572 - val_loss: 1.1558\n",
      "Epoch 79/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2670 - val_loss: 1.1453\n",
      "Epoch 80/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2559 - val_loss: 1.1351\n",
      "Epoch 81/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2280 - val_loss: 1.1251\n",
      "Epoch 82/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2180 - val_loss: 1.1154\n",
      "Epoch 83/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2004 - val_loss: 1.1060\n",
      "Epoch 84/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2086 - val_loss: 1.0969\n",
      "Epoch 85/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1748 - val_loss: 1.0879\n",
      "Epoch 86/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1990 - val_loss: 1.0792\n",
      "Epoch 87/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1780 - val_loss: 1.0708\n",
      "Epoch 88/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1602 - val_loss: 1.0626\n",
      "Epoch 89/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1345 - val_loss: 1.0546\n",
      "Epoch 90/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1308 - val_loss: 1.0468\n",
      "Epoch 91/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1168 - val_loss: 1.0392\n",
      "Epoch 92/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1023 - val_loss: 1.0318\n",
      "Epoch 93/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1095 - val_loss: 1.0246\n",
      "Epoch 94/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0834 - val_loss: 1.0176\n",
      "Epoch 95/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0742 - val_loss: 1.0107\n",
      "Epoch 96/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0780 - val_loss: 1.0040\n",
      "Epoch 97/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0461 - val_loss: 0.9975\n",
      "Epoch 98/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0335 - val_loss: 0.9912\n",
      "Epoch 99/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0456 - val_loss: 0.9850\n",
      "Epoch 100/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0413 - val_loss: 0.9789\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "['ALERT']\n",
      "['OK']\n",
      "['OK']\n",
      "['OK']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load CSV data into a Pandas DataFrame\n",
    "df = pd.read_csv('MOCK_DATA.csv')\n",
    "\n",
    "# Preprocess data and extract relevant features\n",
    "def preprocess_data(df):\n",
    "    processed_data = []\n",
    "    label_encoder = LabelEncoder()\n",
    "    for index, row in df.iterrows():\n",
    "        # Example: Extract relevant features and create a feature vector\n",
    "        encoded_channel = label_encoder.fit_transform([row['channel']])[0]\n",
    "        encoded_cardAcceptorId = label_encoder.fit_transform([row['cardAcceptorId']])[0]\n",
    "        feature_vector = [\n",
    "            float(row['transactionAmount']),\n",
    "            int(row['dateTimeTransaction']),\n",
    "            int(row['merchantCategoryCode']),\n",
    "            encoded_cardAcceptorId,\n",
    "            float(row['cardBalance']),\n",
    "            encoded_channel\n",
    "            # Add more features as needed\n",
    "        ]\n",
    "        processed_data.append(feature_vector)\n",
    "    return np.array(processed_data)\n",
    "\n",
    "# Convert DataFrame to feature matrix\n",
    "X = preprocess_data(df)\n",
    "\n",
    "# Initialize and fit Isolation Forest model\n",
    "iforest_model = IsolationForest(contamination=0.01, random_state=42)\n",
    "iforest_model.fit(X)\n",
    "\n",
    "# Initialize and fit ECOD model\n",
    "ecod_model = ECOD(contamination=0.01)\n",
    "ecod_model.fit(X)\n",
    "\n",
    "# Initialize and fit AutoEncoder model\n",
    "autoencoder_model = AutoEncoder(hidden_neurons=[5, 3, 1, 3, 5], contamination=0.01)\n",
    "autoencoder_model.fit(X)\n",
    "\n",
    "# Initialize and fit LOF model\n",
    "lof_model = LOF(contamination=0.01)\n",
    "lof_model.fit(X)\n",
    "\n",
    "# Load JSON data from file\n",
    "with open('test_data.json', 'r') as json_file:\n",
    "    dataTest = json.load(json_file)\n",
    "\n",
    "# Define the CSV file name and fieldnames\n",
    "csv_file = 'dataTest.csv'\n",
    "fieldnames = list(dataTest.keys())\n",
    "\n",
    "# Write JSON data to CSV file\n",
    "with open(csv_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerow(data)\n",
    "\n",
    "\n",
    "df1 = pd.read_csv('dataTest.csv')\n",
    "Y = preprocess_data(df1)\n",
    "\n",
    "# Predict outliers/anomalies using the trained model\n",
    "iforest_predictions = iforest_model.predict(Y)\n",
    "ecod_predictions = ecod_model.predict(Y)\n",
    "autoencoder_predictions = autoencoder_model.predict(Y)\n",
    "lof_predictions = lof_model.predict(Y)\n",
    "\n",
    "iforest_labels = np.where(iforest_predictions == 1, \"OK\", \"ALERT\")\n",
    "ecod_labels = np.where(ecod_predictions == 1, \"OK\", \"ALERT\")\n",
    "autoencoder_labels = np.where(autoencoder_predictions == 1, \"OK\", \"ALERT\")\n",
    "lof_labels = np.where(lof_predictions == 1, \"OK\", \"ALERT\")\n",
    "\n",
    "print(iforest_labels)\n",
    "print(ecod_labels)\n",
    "print(autoencoder_labels)\n",
    "print(lof_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "29fb7ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# Load JSON data from file\n",
    "with open('test_data.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Define the CSV file name and fieldnames\n",
    "csv_file = 'data.csv'\n",
    "fieldnames = list(data.keys())\n",
    "\n",
    "# Write JSON data to CSV file\n",
    "with open(csv_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351fd025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
